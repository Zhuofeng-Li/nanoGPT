{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 1. block size ? 2. how to produce the input? 3. vocab size?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2652d2fd18a017e2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T08:51:29.356596Z",
     "start_time": "2024-04-04T08:51:24.623328600Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input \n",
    "(B, T) -> (B, T, C)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43f6d8fedef72d3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dropout \n",
    "+ **During training**, randomly zeroes some of the elements of the input tensor with probability p.\n",
    "+ the outputs are scaled by a factor $\\frac{1}{1-p}$ of during training. This means that during evaluation the module simply computes an identity function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "249e46674aebc399"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.2737e+00, -1.5830e+00, -5.7301e-02,  6.6911e-01,  2.0291e+00,\n          7.4391e-01, -7.2854e-01,  0.0000e+00, -3.7143e-01, -9.9263e-02,\n          0.0000e+00,  1.8499e+00, -2.0506e+00, -1.4774e+00,  1.2005e+00,\n         -5.3075e-01],\n        [-1.5099e+00, -4.1922e-01,  1.1408e+00,  4.6337e-01, -0.0000e+00,\n         -5.4516e-01,  0.0000e+00,  1.2371e+00,  0.0000e+00,  0.0000e+00,\n         -2.4196e+00, -3.4048e+00,  5.7232e-01,  4.9851e-01,  0.0000e+00,\n          7.1118e-01],\n        [-1.7112e+00,  2.9942e-02, -1.3743e+00,  5.6715e-01,  2.7618e-01,\n          2.0464e+00, -5.5915e-01,  7.9842e-01, -4.4031e-01,  2.0749e-01,\n         -0.0000e+00,  1.7583e+00,  1.9439e+00, -1.8261e+00, -5.8562e-01,\n         -0.0000e+00],\n        [ 0.0000e+00,  0.0000e+00,  7.8681e-01,  2.7161e+00, -6.0087e-01,\n          2.3701e+00,  1.9097e-02,  1.7065e+00,  4.5573e-01,  8.6003e-01,\n          3.0200e-01,  5.0502e-01,  9.8682e-01, -9.2199e-01, -3.1990e-01,\n         -1.2229e-01],\n        [-1.9405e+00,  0.0000e+00, -0.0000e+00, -2.3400e+00,  5.5097e-01,\n          1.2266e+00, -5.4098e-01, -0.0000e+00,  1.2666e+00, -0.0000e+00,\n          2.0131e+00,  5.1974e-01,  8.5401e-01, -1.6724e+00,  1.9447e+00,\n          0.0000e+00],\n        [-2.9532e-02,  9.1871e-01,  1.0211e+00, -9.9159e-01,  1.0175e+00,\n         -3.3039e-01, -0.0000e+00,  0.0000e+00, -0.0000e+00,  1.0301e+00,\n         -9.6427e-01,  0.0000e+00, -2.2436e+00, -0.0000e+00, -1.7040e+00,\n         -1.7137e+00],\n        [-1.1549e+00, -0.0000e+00,  0.0000e+00, -1.3055e+00,  3.0476e+00,\n          0.0000e+00,  6.4449e-01,  0.0000e+00, -6.1559e-01, -0.0000e+00,\n         -2.4576e-01, -0.0000e+00,  3.8938e-01,  3.4086e-03, -0.0000e+00,\n         -0.0000e+00],\n        [-8.6103e-01, -0.0000e+00, -2.8546e-01, -2.7642e+00,  1.6215e+00,\n         -1.6949e-01,  2.3201e-01, -1.2094e+00, -0.0000e+00,  7.5869e-01,\n         -5.6018e-01,  1.5745e+00,  1.3669e+00,  6.4438e-01,  6.7689e-01,\n         -0.0000e+00],\n        [ 0.0000e+00, -6.4705e-01,  1.7147e-01,  2.1013e+00,  3.4909e-01,\n          7.7420e-02,  4.7826e-01, -0.0000e+00,  1.7669e-02,  0.0000e+00,\n         -4.3277e-01,  5.8424e-01, -1.0418e-01, -1.3133e+00, -2.0251e+00,\n          2.8454e+00],\n        [-2.1967e+00, -1.8084e+00,  0.0000e+00, -1.3542e+00,  0.0000e+00,\n         -2.3669e+00,  6.4192e-01, -5.8991e-01, -0.0000e+00, -0.0000e+00,\n         -0.0000e+00,  3.0218e-01,  1.3008e+00, -2.2643e-02,  3.4813e-01,\n          0.0000e+00],\n        [-0.0000e+00,  0.0000e+00, -7.5140e-01, -0.0000e+00,  6.6704e-01,\n          1.4475e+00,  0.0000e+00, -0.0000e+00,  6.0551e-01,  2.6375e-01,\n         -2.1090e+00, -0.0000e+00,  1.2033e+00,  1.4784e-01,  7.9751e-01,\n         -8.8760e-01],\n        [ 1.0202e+00, -1.1753e+00, -7.9981e-02, -1.0645e+00,  1.4437e+00,\n         -6.8394e-01, -2.0135e-01, -1.8310e+00,  9.4484e-01,  1.3907e+00,\n         -5.0118e-01, -8.4283e-01, -0.0000e+00, -2.0523e-01, -0.0000e+00,\n         -1.6480e-01],\n        [-1.4052e-01, -3.4978e-01,  1.4281e+00, -9.3585e-01, -0.0000e+00,\n          7.5865e-01, -1.8569e+00,  1.9439e+00,  0.0000e+00,  2.1397e-01,\n         -9.3627e-01, -1.2183e+00, -0.0000e+00, -2.2790e+00, -2.5540e+00,\n          3.0507e-01],\n        [-0.0000e+00, -7.7811e-01, -6.6711e-01,  8.2793e-02, -2.8118e+00,\n         -1.5675e+00, -1.7738e+00, -4.7188e-02,  1.6954e+00,  1.3069e-01,\n          7.4755e-01, -3.4008e-01,  1.0221e-01,  1.7936e+00,  4.8661e-01,\n         -1.2091e+00],\n        [ 1.4591e-02, -9.3704e-02,  6.2296e-01, -1.0241e+00, -1.2139e+00,\n         -7.9411e-01,  1.1678e+00, -0.0000e+00,  1.6373e-01,  3.0240e-02,\n          0.0000e+00,  6.8105e-01, -9.6400e-03,  0.0000e+00,  3.9480e-01,\n          2.1825e+00],\n        [ 0.0000e+00,  2.0353e+00, -1.1214e+00, -1.0098e+00, -1.4569e+00,\n         -6.0432e-01,  1.9647e-01, -2.5072e-01, -1.8676e+00,  3.6945e-01,\n          0.0000e+00, -1.0246e+00, -6.4929e-01,  0.0000e+00, -0.0000e+00,\n         -2.1561e-01],\n        [-0.0000e+00,  0.0000e+00,  1.3339e+00,  7.7018e-01, -0.0000e+00,\n         -4.0001e+00, -1.5494e-01, -0.0000e+00,  3.6869e-01, -1.5458e+00,\n         -1.6075e-01, -1.0059e+00,  1.3508e+00,  7.4113e-01,  0.0000e+00,\n          1.2701e+00],\n        [ 1.3517e+00, -6.3124e-01, -2.3924e+00,  2.6584e+00, -6.8512e-01,\n          1.5822e-01,  6.5705e-01, -0.0000e+00,  0.0000e+00,  5.7774e-01,\n         -2.6663e-01,  1.0696e+00,  5.1845e-01, -0.0000e+00, -4.6021e-01,\n         -0.0000e+00],\n        [-0.0000e+00,  2.2987e+00, -1.1603e+00,  3.3339e-01,  0.0000e+00,\n          6.0719e-01, -6.1795e-01,  3.6791e-01,  2.9654e-01,  1.2305e+00,\n          0.0000e+00,  5.4912e-01, -3.9490e-01, -2.2293e-01,  8.7106e-02,\n         -0.0000e+00],\n        [ 1.2877e+00,  2.1705e+00,  0.0000e+00,  7.0909e-02, -1.5923e+00,\n         -1.2492e+00, -0.0000e+00,  1.1915e+00,  3.3750e-01, -5.1710e-01,\n         -1.9024e-01,  4.4501e-02,  1.5557e+00, -4.7794e-02,  2.3049e+00,\n         -1.4276e+00]])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Dropout(p=0.2)\n",
    "input = torch.randn(20, 16)\n",
    "m(input)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T08:51:39.385508900Z",
     "start_time": "2024-04-04T08:51:39.313474700Z"
    }
   },
   "id": "612164820f15423e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Self Attention\n",
    "+ self attention: q, k, v source is x\n",
    "+ **Attention is a communication mechanism**. This can be seen as  nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. However, **node 1 can't accesses 2 and 3**.\n",
    "+  Scaled\" attention: $\\frac{sim}{\\sqrt k}$. This makes it so when input Q,K are unit variance, wei will be **unit variance too and Softmax will stay diffuse and not saturate too much**.\n",
    "\n",
    "![](assets/comm.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4189fef030fcb54d"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m1337\u001B[39m)\n\u001B[0;32m      3\u001B[0m B,T,C \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m,\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m32\u001B[39m \u001B[38;5;66;03m# batch, time, channels\u001B[39;00m\n\u001B[0;32m      4\u001B[0m head_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m16\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Self Attention\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "head_size = 16\n",
    "x = torch.randn(B,T,C)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "q = query(x)  # B, T, h\n",
    "k = query(x)  # B, T, h\n",
    "v = query(x)  # B, T, h\n",
    "\n",
    "sim = torch.matmul(q, k.transpose(-1, -2))  # B, T, T\n",
    "\n",
    "sim = F.softmax(sim, dim=-1)\n",
    "\n",
    "# mask some weight for special cases\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = sim.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = torch.matmul(wei, v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T02:44:29.543727700Z",
     "start_time": "2024-04-04T02:44:28.552530500Z"
    }
   },
   "id": "b9a019fdbcc2cbc6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## difference between layernorm and batchnrom\n",
    "![](assets/norm.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84f3c4a7ea8476f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transformer Framework\n",
    "+ word embedding + position embedding\n",
    "+ attention block: layer norm + attention + MLP\n",
    "+ Layer Nom\n",
    "+ classifier\n",
    "\n",
    "![](assets/transformer.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fec2774332998f44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1d3a739aeed01afc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
